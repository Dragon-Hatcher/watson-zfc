# Logic

In this chapter we will develop the formal language of first order logic upon
which we will build mathematics. Although the objects of our study will be sets
it is first order logic which allows us to make statements about those sets.

First order logic is itself an extension of a simpler system called propositional
logic. Propositional logic encompasses all the logical statements that do not 
involve the quantifiers $\exists$ and $\forall$. This include and, or, not,
and implications.

## Propositional Logic

### Formal Language

When defining propositional logic we will eventually want to be able to express
all of and, or, not, and implies. One possibility is to make all four operators
part of our formal language. However, it turns out that and, or, and not can
all be defined in terms of only implication and a constant _falsum_, which we 
will denote as $\bot$. The advantage of doing things this way is that later we 
will need fewer axioms because we will have fewer foundational terms we need to
define the meaning of. The disadvantage is that to define and and or in terms
of implication we will have to assume to law of the excluded middle or 
equivalently the law of double negation. This means that we will can only do
classical logic. However, supporting intuitionist logic is not a goal of this
development. So we will take the simpler formal language.

The first piece of formal language syntax we want to define is the implication
operator. Below is the Watson command to define the operator. Watson calls
mathematical statements sentences and so defines the non-terminal that 
represents mathematical statements `sentence`. The syntax 
`non-terminal ::= replacement` for defining productions in a context free 
grammar is actually standard and is called Backus–Naur form. The `(500)` 
is a Watson specific extension and allows defining the precedence of different
pieces of syntax. This is not mathematical meaningful but simply helps with
writing notation. For example, we want `¬p → q` to mean `(¬p) → q` and not
`¬(p → q)`.

syntax implication
    sentence (500) ::= sentence " → " sentence
end

The next piece of syntax we will define is the constant symbol _falsum_. This
represents a statement that is always false. The utility of this will become
apparent shortly.

syntax falsum
    sentence ::= "⊥"
end

This is the entire formal language of propositional logic. It is remarkably
small yet it is capable of expressing all quantifier free logical statements.

### Other Logical Operators

Although only implication and falsum are part of our formal language, we still
need to be able to discuss and, or, and not. To do this we will use another
Watson feature: definitions.

Definitions allow as to define new notation along with a replacement rule to 
explain what the definition means. Internally, Watson expands definitions into
the underlying formal language before checking proofs. So definitions have no
effect on proof checking. Instead they are simply a convenience for writing 
proofs. In theory, we do not need to employ definitions at all.

Let us make our first definition in order to define the negation operator. How
can we express negation using only implication and falsum? If a proposition $p$
leads to a contradiction, then we can conclude that $p$ is false. We can write
$p$ leading to a contradiction as $p \to \bot$ since $\bot$ represents a false
statement, a.k.a. a contradiction. So since we know that $p \to \bot$ means 
$p$ is false, we can make this our definition of $\neg p$.

In order to explain this definition to Watson there are two steps. First we 
must define the actual syntax of the notation we want to use.

notation not
    sentence (600) ::= "¬" sentence
end

This allows Watson to correctly parse the notation when we later write it. But
we also need to define what the notation means. We can do this with a 
replacement rule. The reason these are separated into two different steps is 
that often we use the same notation to refer to multiple different things. For
example, later we will use `|x|` to mean both numeric absolute value and set 
cardinality.

definition
    ¬p := p → ⊥
end

The definition syntax is intuitive. You simply place the expression you want
to define on the left, and the definition on the right. Where we wrote `sentence`
in the notation command we give a variable name `p` in the definition command. 
We use `:=` instead of  `=` because `=` is a part of our formal language that we 
will latter want to define. Instead we use `:=` as a metalogical symbol for 
definitions. Note that this means it is not a good idea to make `:=` a symbol
in your formal language.

Next we can define or. Again we must think in terms of the operations we have
already defined. One property of or that we know is that if $p \lor q$ is true 
and $p$ is not true, then $q$ is true. In fact, we can use this as our definition,
defining $p \lor q$ as $\neg p \to q$.

notation or
    sentence ::= sentence " ∨ " sentence
end

definition
    p ∨ q := ¬p → q
end

Notice that the definition is not symmetric. When we prove the properties of or
we will initially have to have separate proofs for the left and right sides.

We can define and in terms of or using the standard DeMorgan's Law relationship.
That is we define $p \land q$ to mean $\neg(\neg p \lor \neg q)$. However,
before we can make this definition we need one other definition: parentheses!
Don't laugh. Parentheses are a part of mathematical notation too and they need
a definition just like everything else. Of course, the definition is rather simple.

notation parentheses
    sentence ::= "(" sentence ")"
end

definition
    (p) := p
end

With this we are able to define and.

notation and
    sentence ::= sentence " ∧ " sentence
end

definition
    p ∧ q := ¬(¬p ∨ ¬q)
end

There is one final definition we would like to make: $p \leftrightarrow q$. 
The definition is completely standard however there is one thing to comment on.
Notice that the definition of $p \leftrightarrow q$ includes two copies each
of $p$ and $q$. If $p$ and $q$ themselves contain if and only ifs, then the 
size of the formal language sentence our notation describes would grow
exponentially! Luckily Watson is equipped to handle this and stores the formal
language sentences in a form where the size is linear with respect to the notation
used. 

notation iff
    sentence ::= sentence " ↔ " sentence
end

definition
    p ↔ q := (p → q) ∧ (q → p)
end

### Axioms

Having defined our notation we are now ready to give the axioms of propositional 
logic. Just as we amazingly need only two operators to define the complete syntax 
of propositional logic, we also need just four axioms to completely describe the 
properties of those operators. When we introduce quantifiers we will add two 
more axioms which means we need just six axioms to define all of logic!

The first axiom is the most venerable and most intuitive of the four. It is 
the axiom of modus ponens. It says that if $p \to q$ and $p$ are both true then
$q$ is also true. This might seem so obvious as to barely require stating but
remember from the introduction that it is only the axioms that give the
sentences in our formal language meaning. So far all we have told Watson is that
we are allowed to write the literal symbol `→`. We have not yet told Watson what
this symbol means. The axiom of modus ponens is what gives the implication
symbol its meaning.

axiom modus_ponens [p q : sentence] : (p → q) (p)
    |- q
end

Let's take a moment to understand the `axiom` command. Immediately following the
keyword `axiom` is the name of the axiom. When we later write proofs we will
invoke the axiom by referring to this name. Next comes the _templates_ of the axiom.
When we say that $p \to q$ and $p$ together imply $q$ we mean that this 
deduction holds for any possible $p$ and $q$. It is this idea of the statement
being true for any possible $p$ and $q$ that the syntax `[p q : sentence]`
tells Watson. After the templates comes a `:` to indicate the end of the 
templates and the beginning of the hypotheses. Finally, we mark the conclusion
of the axiom with the `|-` symbol. This is a standard symbol from logic called
a _tack_ or _turnstile_ and can be read as "entails." So we get that the 
hypotheses entail the conclusion.

In the axiom the implication has been eliminated from the hypothesis to reach
the conclusion. As such the axiom of modus ponens is also often called the axiom
of implication elimination. Modus ponens allows us to derive a new fact if we 
already know an implication. But how will we learn an implication in the first
place? This will be accomplished with the second axiom of propositional logic:
the axiom of implication introduction or, as we will call it, the deduction axiom.

axiom deduction [p q : sentence] : (assume p |- q)
    |- p → q
end

This axiom has a hypothesis which is quite different from the ones we have seen
before. You will notice that `assume p |- q` is neither part of our formal
language nor a piece of notation we have defined. In fact it is not a 
mathematical statement at all, it is a *meta*mathematical statement. When we
write `assume p |- q` as a hypothesis, what we are telling Watson is that the 
conclusion can be reached if knowing that $p$ was true would allow us to prove 
that $q$ was true. Thus `assume p |- q` is not a statement that has a truth 
value within the mathematical system we are defining. It is not even a statement
in that system! This is a question about the behavior of the proof system itself.
Therefore it is *meta*mathematical.

In Watson we call statements of the form `assume p |- q` _facts_ to distinguish
them from statements in our formal mathematical system which you will recall we
are calling _sentences_. Watson will record which facts have been proven and
facts can be hypotheses but a fact cannot be a conclusion of an axiom. To do so
would be a category error. An axiom tells Watson about the definition of truth
in the formal language we are defining. However it is Watson itself that decides
which things are provable. To make a fact the conclusion of an axiom would be to
declare that a certain proof exists, which might not be true. Whether such a
proof exists is a concrete fact about the world, not an abstract truth function
we can define specific cases of.

The final two axioms concern the falsum constant. The third axiom is the axiom of 
double negation. It says that if $\neg\neg p$ is true then $p$ is also true.
It is the addition of this deduction that makes our logic classical. Because we
defined and and or in terms of implication and falsum this axiom is required to
prove facts about and and or. So almost no intuitionist reasoning will be 
possible in our system.

axiom not_not.elim [p : sentence] : (¬¬p)
    |- p
end

The fourth and final axiom is the principal of explosion. It states that if we
know $\bot$ to be true, anything at all can be proven.

axiom explosion [p : sentence] : (⊥)
    |- p
end

This axiom may seem surprising but it would actually be implied by any system of
reasoning about falsity. If we know that $q$ and $\neg q$ are both true then we
can make the following (right now informal) argument that an arbitrary $p$ is
true. Since $q$ is true $q \lor p$ is true. But $\neg q$ is true so since 
$q \lor p$ is true it must be that $p$ is true. Hence $p$. As we can see the
principal of explosion will be true once we have defined all the normal logical
deductions anyway. So, since it turns out that the principal of explosion will
help us prove those deductions, it is reasonable to add it as an axiom.

### Tactics

With our definitions and axioms in hand, we are now ready to begin proving
theorems. Theorem statements in Watson look identical to axioms except, of 
course, that they come with a proof. What is the syntax for these proofs? 
The syntax is entirely defined by us! Watson has no built in proof syntax.
Instead we use the `tactic` command to define a new type of proof syntax and 
then write code in the scripting language Lua that transforms this proof syntax
into a formal deduction that Watson can check. Let's define a simple proof syntax
we can use to prove our first theorems.

The simplest proof step is claiming that a fact is true and providing a
justification for that step. There are multiple ways of justifying a fact so we
will create a non-terminal symbol that represents justifications. We can do this
with the `tactic_category` command.

tactic_category justification

Then we will claim that a fact is true by using the `follows` tactic. Notice
that we provide both the justification for the given fact and the tactic that
will complete the rest of the proof given this new fact.

tactic tactic.follows
    -- The name before the colon is the name that can be used in Lua to access 
    -- the sub-tactic. The name after the colon is the non-terminal.
    tactic ::= @kw"follows" goal:@fact just:justification ";" next:tactic
end

One form of justification for a fact is applying a theorem. We will call this
justification `by`. To use a theorem we need to fill in any template parameters
the theorem has. We will create a new non-terminal for the templates.

tactic_category templates

tactic justification.by
    justification ::= @kw"by" thm:@name tmps:templates
end

And then we will fill in the syntax for a list of template instantiations.

tactic_category template

tactic templates.some
    templates ::= tmp:template rest:templates
end

tactic templates.none
    templates ::=
end

tactic template
    template ::= "[" frag:@any_fragment "]"
end

The pattern of a `some` and `none` variant is a common way of expressing a list
within a context free grammar.

Another possible justification is using a whole subproof to prove a given fact.
We will call this justification `as` which reminds us of a proof remark like
"the fact follows as this subproof demonstrates."

tactic justification.tactic
    justification ::= @kw"as" tactic:tactic
end

Another useful tactic will be `todo`. This can be used when we are in the middle 
of writing our proofs. It will prove anything we want but Watson will mark our
proof as unfinished so we know to return and finish it later. This is similar to
`sorry` in lean.

tactic tactic.todo
    tactic ::= @kw"todo"
end

The final tactic we will need is the simplest tactic of all: do nothing. This is
useful if the goal of our proof has already been proven and we don't need to do
anything more.

tactic tactic.empty
    tactic ::=
end

### And and Or

We will start by proving the basic facts about or. For or there are two 
introduction rules. Left and right introduction. That is, if we know $p$ is true
then we know $p \lor q$ is true. And also if we know $q$ is true then $p \lor q$
is true. As we mentioned before, the definition of or is non-symmetrical so the
proofs of these two facts are different.

theorem or.intro_right [p q : sentence] : (q)
    |- p ∨ q
proof
    -- p ∨ q means ¬p → q by definition. so we want to prove an implication.
    -- we will start by assuming that ¬p is true.
    follows assume ¬p |- q as
        -- however q is a hypothesis so there is nothing to be proved
    ;

    -- we have assume ¬p |- q but we want ¬p → q so we will use the deduction
    -- theorem to convert between these
    follows ¬p → q by deduction [¬p] [q];
qed

This proof features a very common style of deduction: we assume $p$ and
deduce $q$. Then we apply the deduction theorem to conclude $p \to q$. Let's
add a new tactic that automatically performs this reasoning.

tactic tactic.assume_then
    tactic ::= @kw"assume" assumption:@fragment(sentence) @kw"then" 
               conclusion:@fragment(sentence) just:justification ";" next:tactic
end

We can use this new tactic to prove the other or introduction rule.

theorem or.intro_left [p q : sentence] : (p)
    |- p ∨ q
proof
    -- since p ∨ q is really ¬p → q it suffices to show ¬p → q.
    assume ¬p then q as
        -- we have both p and ¬p which is a contradiction. recall that ¬p means
        -- p → ⊥ so we can derive ⊥ and then use explosion to get q.
        follows ⊥ by modus_ponens [p] [⊥];
        follows q by explosion [q];
    ;
qed

In this proof we see two more common proof patterns that we will make into
tactics. If we can derive $\bot$ then our proof is always done because we can
use the explosion theorem. This represents the normal proof idea of having 
reached a contradiction. The `contradiction` tactic will automatically search
the facts we have proven to see if we have both a fact and its negation. If it
finds such a contradiction it will complete the proof. Otherwise it will produce
an error.

tactic tactic.contradiction
    tactic ::= @kw"contradiction"
end

The other proof pattern we observed came at the start. We saw that $p \lor q$
was defined as $\neg p \to q$ so it suffices to show this second fact. We wrote
a comment to explain this but it is better to have Watson actually check the 
equivalence for us. We will introduce a new tactic `suffices` that changes the
goal to a new goal using the given justification to show that the new goal is
sufficient to prove the old goal.

tactic tactic.suffices
    tactic ::= @kw"suffices" goal:@fragment(sentence) just:justification ";" 
               next:tactic
end

If the two goals are definitionally equal, as they were in `or.intro_left`, no
justification is needed. Let's add a new, empty, justification for when there
isn't actually anything to prove.

tactic justification.empty
    justification ::=
end

Armed with these new tactics we can prove the elimination rule for or. It says
that if we know $p \lor q$ is true and that both $p$ and $q$ imply $r$ then $r$
is also true. This is a natural inference rule and we could have chosen this as
an axiom instead of double negation elimination.

theorem or.elim [p q r : sentence] : (p ∨ q) (p → r) (q → r)
    |- r
proof
    -- assuming ¬r will lead to a contradiction. thus we will have ¬¬r which
    -- we can convert to r using the not_not.elim axiom.
    assume ¬r then ⊥ as
        -- p gives us r which is a contradiction. so we have ¬p
        assume p then ⊥ as
            follows r by modus_ponens [p] [r];
            contradiction
        ;
        -- but since p ∨ q we must then have q
        follows q by modus_ponens [¬p] [q];
        -- which also gives us r
        follows r by modus_ponens [q] [r];
        -- so we have a contradiction still
        contradiction
    ;
    follows r by not_not.elim [r];
qed

Once again there is a common proof pattern here we can make into a tactic. We
assume a fact and then derive a contradiction from it. This allows us to 
conclude the negation of that fact. We will call the tactic `assume for_contra`.

tactic tactic.assume_contra
    tactic ::= @kw"assume" assumption:@fragment(sentence) @kw"for_contra" ";" 
               next:tactic
end

Also, and this is an entirely aesthetic concern, writing `follows` everywhere is
boring. Let's introduce `thus` which does the same thing as `follows` but is
easier to read when one fact follows directly from what was previously shown.

tactic tactic.thus
    tactic ::= @kw"thus" goal:@fact just:justification ";" next:tactic
end

We are now ready to prove that or is commutative.

theorem or.comm [p q : sentence] : (p ∨ q)
    |- q ∨ p
proof
    assume p then q ∨ p by or.intro_right [q] [p];
    assume q then q ∨ p by or.intro_left [q] [p];
    thus q ∨ p by or.elim [p] [q] [q ∨ p];
qed

If we know $p \lor q$ and that $p$ is false then we must have $q$. Once again,
since the definition of or is asymmetric the two proofs are not the same. 

theorem or.not_left [p q : sentence] : (p ∨ q) (¬p)
    |- q
proof
    -- this theorem is how we defined or! p ∨ q is by definition ¬p → q
    follows ¬p → q;
    thus q by modus_ponens [¬p] [q];
qed

Just like with `suffices` it is helpful to have Watson check if our goal is
definitionally equal, let's add the `have` tactic when we are claiming a fact
we already have is definitionally equal to a new form.

tactic tactic.have
    tactic ::= @kw"have" f1:@fragment(sentence) @kw"from" f2:@fragment(sentence)
               ";" next:tactic
end

We can apply `or.not_left` to prove `or.not_right`.

theorem or.not_right [p q : sentence] : (p ∨ q) (¬q)
    |- p
proof
    assume ¬p for_contra;
    thus q by or.not_left [p] [q];
    contradiction
qed

Next let's move on to proving properties about and. Whereas or has two 
introduction rules and one elimination rule, and has one introduction rule and
two elimination rules.

theorem and.intro [p q : sentence] : (p) (q)
    |- p ∧ q
proof
    -- we can expand the definition of p ∧ q
    suffices ¬(¬p ∨ ¬q);
    assume ¬p ∨ ¬q for_contra;
    -- but either of these possibilities leads to a contradiction
    assume ¬p then ⊥ as contradiction;
    assume ¬q then ⊥ as contradiction;
    thus ⊥ by or.elim [¬p] [¬q] [⊥];
qed

This time the definition of and is symmetrical so the proofs of the two 
elimination rules are the same.

theorem and.elim_left [p q : sentence] : (p ∧ q)
    |- p
proof
    assume ¬p for_contra;
    thus ¬p ∨ ¬q by or.intro_left [¬p] [¬q];
    have ¬(¬p ∨ ¬q) from p ∧ q;
    contradiction
qed

theorem and.elim_right [p q : sentence] : (p ∧ q)
    |- q
proof
    assume ¬q for_contra;
    thus ¬p ∨ ¬q by or.intro_right [¬p] [¬q];
    have ¬(¬p ∨ ¬q) from p ∧ q;
    contradiction
qed

Then the proof of and commutativity is very simple.

theorem and.comm [p q : sentence] : (p ∧ q)
    |- q ∧ p
proof
    follows p by and.elim_left [p] [q];
    follows q by and.elim_right [p] [q];
    thus q ∧ p by and.intro [q] [p];
qed

### If And Only If

Since if and only if is just a special case of and it has the same introduction
and elimination rules. We just refer to the theorems we proved for and.

theorem iff.intro [p q : sentence] : (p → q) (q → p)
    |- p ↔ q
proof
    suffices (p → q) ∧ (q → p);
    follows (p → q) ∧ (q → p) by and.intro [p → q] [q → p];
qed

theorem iff.elim_fwd [p q : sentence] : (p ↔ q)
    |- p → q
proof
    follows p → q by and.elim_left [p → q] [q → p];
qed

theorem iff.elim_rev [p q : sentence] : (p ↔ q)
    |- q → p
proof
    follows q → p by and.elim_right [p → q] [q → p];
qed

And we can also prove a few more standard properties.

theorem iff.mp [p q : sentence] : (p ↔ q) (p)
    |- q
proof
    follows p → q by iff.elim_fwd [p] [q];
    thus q by modus_ponens [p] [q];
qed

theorem iff.mpr [p q : sentence] : (p ↔ q) (q)
    |- p
proof
    follows q → p by iff.elim_rev [p] [q];
    thus p by modus_ponens [q] [p];
qed

Sometimes if $p \leftrightarrow q$ we say that $p$ and $q$ are equivalent. We
can make this notion rigorous in the following sense: for any property that 
holds of $p$, that same property also holds of $q$ (and vice versa). We can 
show that this equivalence holds by induction over all possible formal sentences
containing $p$.

theorem iff.equiv.imp1 [p q r : sentence] : (p ↔ q) (p → r)
    |- q → r
proof
    assume q then r as
        follows p by iff.mpr [p] [q];
        thus r by modus_ponens [p] [r];
    ;
qed

theorem iff.equiv.imp2 [p q r : sentence] : (p ↔ q) (r → p)
    |- r → q
proof
    assume r then q as
        follows p by modus_ponens [r] [p];
        thus q by iff.mp [p] [q];
    ;
qed

theorem iff.equiv.falsum [p q : sentence] : (p ↔ q) (⊥)
    |- ⊥
proof
    -- not much to prove... 
qed

With these three proofs we can see that for any property $m(p)$, if 
$p \leftrightarrow q$ then we will also have $m(q)$. To complete the proof we
would have to perform induction over all possible properties $m$. However such
an induction would have to take place outside of the formal system. In any 
specific case we could prove it as a theorem, but the general case is a
*meta*theorem. Watson is not capable of proving such metatheorems, so we will
have to add it as an axiom. But we should remember that our reasons for 
adding this axiom are different than our reasons for the other axioms.

To write the axiom we will need some way of denoting a property of $p$.

notation sentence.prop
    sentence ::= @name "[" sentence "]"
end

Then we can state the axiom, which we will call `iff.rw` for if and only if
rewrite.

axiom iff.rw [p q m[_] : sentence] : (p ↔ q) (m[p])
    |- m[q]
end
